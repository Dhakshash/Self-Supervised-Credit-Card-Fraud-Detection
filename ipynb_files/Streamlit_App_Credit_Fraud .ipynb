{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48NH6ekFL0m1",
    "outputId": "2c3a4900-a31f-4b45-e398-1f804e0f5afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install streamlit pyngrok --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHlRsZVEL9PY",
    "outputId": "c647a521-0274-4d01-9bf4-f544ddcd41e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2fLbVi7LCc0"
   },
   "outputs": [],
   "source": [
    "code_blocks = \"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score, average_precision_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# App\n",
    "# ------------------------------\n",
    "st.set_page_config(page_title=\"Credit Card Fraud Detection\", layout=\"wide\")\n",
    "st.title(\"üïµÔ∏è Credit Card Fraud Detection (SCARF Embeddings + Anomaly Detectors)\")\n",
    "\n",
    "# ------------------------------\n",
    "# Sidebar (fixed options)\n",
    "# ------------------------------\n",
    "st.sidebar.header(\"üîß Select Model and Method\")\n",
    "embedding_type = st.sidebar.selectbox(\"Choose SCARF Embedding\", [\"InfoNCE\", \"BarlowTwins\", \"VICReg\"])\n",
    "detector_type  = st.sidebar.selectbox(\"Choose Anomaly Detection Method\", [\"DeepSVDD\", \"Autoencoder\", \"IsolationForest\"])\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a CSV file\", type=[\"csv\"])\n",
    "\n",
    "# ------------------------------\n",
    "# Paths (must match training)\n",
    "# ------------------------------\n",
    "ARTIFACTS_DIR = \"/content/drive/MyDrive/credit_fraud_scarf/artifacts\"\n",
    "\n",
    "ENCODER_PATHS = {\n",
    "    \"InfoNCE\":      os.path.join(ARTIFACTS_DIR, \"scarf_encoder_InfoNCE.pth\"),\n",
    "    \"BarlowTwins\":  os.path.join(ARTIFACTS_DIR, \"scarf_encoder_BarlowTwins.pth\"),\n",
    "    \"VICReg\":       os.path.join(ARTIFACTS_DIR, \"scarf_encoder_VICReg.pth\"),\n",
    "}\n",
    "\n",
    "# Map UI name -> your saved lowercase suffixes\n",
    "EMB_SUFFIX = {\"InfoNCE\": \"infonce\", \"BarlowTwins\": \"barlow\", \"VICReg\": \"vicreg\"}\n",
    "def get_detector_paths(selected_embedding: str):\n",
    "    sfx = EMB_SUFFIX[selected_embedding]\n",
    "    return {\n",
    "        \"DeepSVDD\":        os.path.join(ARTIFACTS_DIR, f\"deepsvdd_center_{sfx}.npy\"),\n",
    "        \"Autoencoder\":     os.path.join(ARTIFACTS_DIR, f\"ae_best_{sfx}.pth\"),\n",
    "        \"IsolationForest\": os.path.join(ARTIFACTS_DIR, f\"iforest_model_{sfx}.pkl\"),\n",
    "    }\n",
    "\n",
    "SCALER_CSV     = os.path.join(ARTIFACTS_DIR, \"scaling_params.csv\")\n",
    "THRESHOLDS_CSV = os.path.join(ARTIFACTS_DIR, \"thresholds.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# Models\n",
    "# ------------------------------\n",
    "class MLPEncoder(nn.Module):\n",
    "    # num_layers=2 to match your training checkpoints\n",
    "    def __init__(self, input_dim=30, hidden_dim=128, num_layers=2, output_dim=128):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            layers += [nn.Linear(in_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU()]\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))  # final projection used for embeddings\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    # matches your checkpoint: 128 -> 64 -> 128\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(dim, 64), nn.ReLU())\n",
    "        self.decoder = nn.Sequential(nn.Linear(64, dim))\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def must_exist(path, label):\n",
    "    if not os.path.exists(path):\n",
    "        st.error(f\"Missing {label}: {path}\")\n",
    "        st.stop()\n",
    "\n",
    "def load_fixed_threshold(thr_csv, emb_name, det_name):\n",
    "    \\\"\\\"\\\"Fixed threshold only (no fallback), tolerant to common aliases.\\\"\\\"\\\"\n",
    "    must_exist(thr_csv, \"thresholds.csv\")\n",
    "    df = pd.read_csv(thr_csv)\n",
    "\n",
    "    # normalize headers\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    col_emb = \"embedding_type\" if \"embedding_type\" in df.columns else (\"embedding\" if \"embedding\" in df.columns else None)\n",
    "    col_met = \"method\"         if \"method\" in df.columns         else (\"detector\" if \"detector\" in df.columns else None)\n",
    "    if not col_emb or not col_met or \"threshold\" not in df.columns:\n",
    "        st.error(\"thresholds.csv must contain columns: embedding_type (or embedding), method (or detector), threshold\")\n",
    "        st.stop()\n",
    "\n",
    "    # alias maps (lowercase)\n",
    "    embed_map = {\"infonce\":\"infonce\",\"barlow\":\"barlowtwins\",\"barlowtwins\":\"barlowtwins\",\"vicreg\":\"vicreg\"}\n",
    "    method_map = {\"deepsvdd\":\"deepsvdd\",\"deep_svdd\":\"deepsvdd\",\"svdd\":\"deepsvdd\",\n",
    "                  \"autoencoder\":\"autoencoder\",\"ae\":\"autoencoder\",\n",
    "                  \"isolationforest\":\"isolationforest\",\"iforest\":\"isolationforest\",\"isoforest\":\"isolationforest\"}\n",
    "\n",
    "    # normalize CSV values\n",
    "    df[col_emb] = df[col_emb].astype(str).str.strip().str.lower().map(lambda x: embed_map.get(x, x))\n",
    "    df[col_met] = df[col_met].astype(str).str.strip().str.lower().map(lambda x: method_map.get(x, x))\n",
    "\n",
    "    # normalize keys from UI\n",
    "    key_emb = embed_map.get(emb_name.lower().strip(), emb_name.lower().strip())\n",
    "    key_met = method_map.get(det_name.lower().strip(), det_name.lower().strip())\n",
    "\n",
    "    row = df[(df[col_emb] == key_emb) & (df[col_met] == key_met)]\n",
    "    if len(row) != 1:\n",
    "        with st.expander(\"Thresholds debug\"):\n",
    "            st.write(\"Unique embeddings:\", sorted(df[col_emb].unique()))\n",
    "            st.write(\"Unique methods:\", sorted(df[col_met].unique()))\n",
    "            st.write(\"Looking for:\", {\"embedding_type\": key_emb, \"method\": key_met})\n",
    "            st.dataframe(df[[col_emb, col_met, \"threshold\"]])\n",
    "        st.error(f\"No fixed threshold found for {emb_name} + {det_name} in thresholds.csv.\")\n",
    "        st.stop()\n",
    "\n",
    "    with st.expander(\"Preview thresholds.csv\"):\n",
    "        st.dataframe(pd.read_csv(thr_csv))\n",
    "        st.write(\"**Using threshold row:**\")\n",
    "        st.dataframe(row.rename(columns={col_emb:\"embedding_type\", col_met:\"method\"})[[\"embedding_type\",\"method\",\"threshold\"]])\n",
    "\n",
    "    return float(row[\"threshold\"].values[0])\n",
    "\n",
    "# ------------------------------\n",
    "# Main flow\n",
    "# ------------------------------\n",
    "if uploaded_file is not None:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "\n",
    "    # scale Time/Amount if scaler csv exists\n",
    "    if os.path.exists(SCALER_CSV):\n",
    "        sc = pd.read_csv(SCALER_CSV)\n",
    "        for col in [\"Time\", \"Amount\"]:\n",
    "            if col in df.columns and col in sc[\"column\"].values:\n",
    "                mean = sc.loc[sc[\"column\"] == col, \"mean\"].values[0]\n",
    "                std  = sc.loc[sc[\"column\"] == col, \"std\"].values[0] or 1.0\n",
    "                df[col] = (df[col] - mean) / std\n",
    "        st.success(\"‚úÖ Data uploaded and standardized.\")\n",
    "    else:\n",
    "        st.info(\"‚ÑπÔ∏è scaling_params.csv not found ‚Äî continuing without scaling.\")\n",
    "\n",
    "    st.write(\"### Preview of Processed Data:\")\n",
    "    st.dataframe(df.head())\n",
    "\n",
    "    # load encoder\n",
    "    enc_path = ENCODER_PATHS[embedding_type]\n",
    "    must_exist(enc_path, f\"{embedding_type} encoder\")\n",
    "    encoder = MLPEncoder()\n",
    "    state = torch.load(enc_path, map_location=\"cpu\")\n",
    "    encoder.load_state_dict(state, strict=True)\n",
    "    encoder.eval()\n",
    "    st.success(f\"‚úÖ Loaded SCARF encoder: {embedding_type}\")\n",
    "\n",
    "    # embeddings\n",
    "    with torch.no_grad():\n",
    "        X = df.drop(columns=[\"Class\"], errors=\"ignore\").values.astype(np.float32)\n",
    "        embeddings = encoder(torch.from_numpy(X)).numpy()\n",
    "    st.success(\"‚úÖ Embeddings generated.\")\n",
    "    st.write(\"### Sample Embeddings (first 5 rows):\")\n",
    "    st.dataframe(pd.DataFrame(embeddings[:5]))\n",
    "\n",
    "    # fixed threshold (no fallback)\n",
    "    threshold = load_fixed_threshold(THRESHOLDS_CSV, embedding_type, detector_type)\n",
    "\n",
    "    # detector scoring (uses lowercase suffix paths)\n",
    "    DETECTOR_PATHS = get_detector_paths(embedding_type)\n",
    "    det_path = DETECTOR_PATHS[detector_type]\n",
    "    must_exist(det_path, f\"{detector_type} model\")\n",
    "\n",
    "    if detector_type == \"IsolationForest\":\n",
    "        clf = joblib.load(det_path)\n",
    "        scores = -clf.decision_function(embeddings)  # higher = more anomalous\n",
    "\n",
    "    elif detector_type == \"DeepSVDD\":\n",
    "        center = np.load(det_path)\n",
    "        if center.shape[0] != embeddings.shape[1]:\n",
    "            st.error(f\"DeepSVDD center dim {center.shape[0]} != embedding dim {embeddings.shape[1]}\")\n",
    "            st.stop()\n",
    "        # squared distance to align with training thresholds\n",
    "        diff = embeddings - center\n",
    "        scores = (diff * diff).sum(axis=1)\n",
    "\n",
    "    elif detector_type == \"Autoencoder\":\n",
    "        ae = AE(dim=embeddings.shape[1])\n",
    "        ae.load_state_dict(torch.load(det_path, map_location=\"cpu\"), strict=True)\n",
    "        ae.eval()\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor(embeddings, dtype=torch.float32)\n",
    "            recon = ae(t)\n",
    "            # ‚úÖ Use SSE per sample (sum of squared error) to match training thresholds\n",
    "            diff = recon - t\n",
    "            scores = (diff * diff).sum(dim=1).cpu().numpy()\n",
    "\n",
    "    # predictions with fixed threshold\n",
    "    df_results = df.copy()\n",
    "    df_results[\"anomaly_score\"] = scores\n",
    "    df_results[\"is_fraud\"] = (df_results[\"anomaly_score\"] > threshold).astype(int)\n",
    "\n",
    "    st.success(f\"‚úÖ Flagged {int(df_results['is_fraud'].sum())} transactions (fixed threshold = {threshold:.6f})\")\n",
    "    st.write(\"### üîç Top-K Flagged Transactions\")\n",
    "    st.dataframe(df_results[df_results[\"is_fraud\"] == 1]\n",
    "                 .sort_values(by=\"anomaly_score\", ascending=False)\n",
    "                 .head(10))\n",
    "\n",
    "    # evaluation (only if labels provided)\n",
    "    if \"Class\" in df.columns:\n",
    "        y_true = df[\"Class\"].values.astype(int)\n",
    "        y_pred = df_results[\"is_fraud\"].values\n",
    "        st.subheader(\"üìä Evaluation Metrics (fixed threshold)\")\n",
    "        st.write(f\"**ROC-AUC:** {roc_auc_score(y_true, scores):.4f}\")\n",
    "        st.write(f\"**PR-AUC:** {average_precision_score(y_true, scores):.4f}\")\n",
    "        st.write(f\"**Precision:** {precision_score(y_true, y_pred, zero_division=0):.4f}\")\n",
    "        st.write(f\"**Recall:** {recall_score(y_true, y_pred, zero_division=0):.4f}\")\n",
    "        st.write(f\"**F1-score:** {f1_score(y_true, y_pred, zero_division=0):.4f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        fig_cm, ax_cm = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax_cm)\n",
    "        ax_cm.set_xlabel(\"Predicted\"); ax_cm.set_ylabel(\"Actual\"); ax_cm.set_title(\"Confusion Matrix\")\n",
    "        st.pyplot(fig_cm)\n",
    "\n",
    "        fig_roc, ax_roc = plt.subplots()\n",
    "        RocCurveDisplay.from_predictions(y_true, scores, ax=ax_roc)\n",
    "        st.pyplot(fig_roc)\n",
    "\n",
    "        fig_pr, ax_pr = plt.subplots()\n",
    "        PrecisionRecallDisplay.from_predictions(y_true, scores, ax=ax_pr)\n",
    "        st.pyplot(fig_pr)\n",
    "\n",
    "    # download predictions\n",
    "    st.download_button(\n",
    "        \"‚¨áÔ∏è Download predictions CSV\",\n",
    "        df_results.to_csv(index=False).encode(\"utf-8\"),\n",
    "        file_name=f\"predictions_{embedding_type}_{detector_type}.csv\",\n",
    "        mime=\"text/csv\",\n",
    "        use_container_width=True\n",
    "    )\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfwH302WgC9y"
   },
   "outputs": [],
   "source": [
    "with open(\"streamlit_app.py\", \"w\") as f:\n",
    "    f.write(code_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmnpbSNzaX0m",
    "outputId": "cfcdadda-2254-424d-cb06-fda0d48029b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to /content/drive/MyDrive/credit_fraud_scarf/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# Save your code to Drive\n",
    "!mkdir -p /content/drive/MyDrive/credit_fraud_scarf\n",
    "!cp /content/streamlit_app.py /content/drive/MyDrive/credit_fraud_scarf/\n",
    "print(\"‚úÖ Saved to /content/drive/MyDrive/credit_fraud_scarf/streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYOcS-wQhHyi"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok --quiet\n",
    "from pyngrok import ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZaq0LDujB9h",
    "outputId": "3822e428-ef1c-496d-d5ff-7a83286d3a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"Your Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dGottefgr4t",
    "outputId": "04b3209d-bc02-49b6-e5ae-59882b795ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Your Streamlit app is live: NgrokTunnel: \"https://e61238f0961f.ngrok-free.app\" -> \"http://localhost:8501\"\n"
     ]
    }
   ],
   "source": [
    "# Kill any previous tunnels\n",
    "!pkill streamlit\n",
    "\n",
    "# Start the app in the background\n",
    "!streamlit run /content/drive/MyDrive/credit_fraud_scarf/streamlit_app.py &> /dev/null &\n",
    "\n",
    "# Wait a moment to let the server start\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Create a public URL using ngrok\n",
    "from pyngrok import ngrok\n",
    "public_url = ngrok.connect(8501)\n",
    "print(f\"üöÄ Your Streamlit app is live: {public_url}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
